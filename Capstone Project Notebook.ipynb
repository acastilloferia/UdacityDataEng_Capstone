{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Engineer Capstone Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek,concat_ws\n",
    "from SAS_Valid_Values import i94cntyl, i94prtl, i94model, i94addrl\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Do all imports and installs here\n",
    "#import pandas as pd\n",
    "output_data = \"outdata/\"\n",
    "\n",
    "# Define Spark Session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Data Wrangling on External Temperatures DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "df3 = pd.read_csv(\"city_temperature.zip\",sep=',',low_memory=False, compression='zip')\n",
    "\n",
    "# Filter for only US cities\n",
    "df3=df3[df3['Country']=='US']\n",
    "\n",
    "# Check Dataset Quality (https://www.kaggle.com/sudalairajkumar/daily-temperature-of-major-cities)\n",
    "\n",
    "# Drop unsued columns and duplicates\n",
    "df3.drop('Region',  axis='columns', inplace=True)\n",
    "df3.drop('Country',  axis='columns', inplace=True)\n",
    "df3=df3.drop_duplicates()\n",
    "\n",
    "# Create a reverse dict from SAS i94addrl\n",
    "i94addrl_r = dict(zip(i94addrl.values(),i94addrl.keys()))\n",
    "\n",
    "# Convert to spark Dataframe\n",
    "df3_spark=spark.createDataFrame(df3)\n",
    "\n",
    "# Define a lookup UDF and create StateCode column based on State name informationÃ§\n",
    "def getState(key, default=None):\n",
    "    if str(key).upper() in i94addrl_r:\n",
    "        return i94addrl_r[str(key).upper()]\n",
    "    return default\n",
    "get_i94addrl_r = udf(lambda x: getState(x))\n",
    "df3_spark = df3_spark.withColumn(\"stateCode\",get_i94addrl_r(df3_spark.State))\n",
    "\n",
    "# Create CityCode with contatenation\n",
    "df3_spark = df3_spark.withColumn('cityCode', sf.concat(sf.col('City'),sf.lit('_'), sf.col('stateCode')))\n",
    "\n",
    "# Group observation date into a single column\n",
    "df3_spark = df3_spark.withColumn('temperatureDate', \n",
    "                    sf.concat(sf.col('Year'),sf.lit('-'), sf.lpad(df3_spark['Month'],2,'0'),\n",
    "                    sf.lit('-'), sf.lpad(df3_spark['Day'],2,'0')))\n",
    "\n",
    "# Remove unused columns\n",
    "df3_spark=df3_spark.drop('State')\n",
    "df3_spark=df3_spark.drop('City')\n",
    "\n",
    "# Remove null values\n",
    "df3_spark=df3_spark.na.drop()\n",
    "\n",
    "# Write to parquet file (pending partition by STATECODE / YEAR / MONTH)\n",
    "df3_spark.write.mode('append').partitionBy(\"stateCode\",\"Year\", \"Month\").parquet(output_data+\"temperatures/temperatures.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Data Wrangling CITIES file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "df = pd.read_csv(\"us-cities-demographics.csv\",sep=';')\n",
    "\n",
    "# Diplay Male Population nulls\n",
    "#df[df.isnull().any(axis=1)]\n",
    "\n",
    "# Fix average HouseHold Size by applying mean value\n",
    "df['Average Household Size'].fillna(df['Average Household Size'].mean(), inplace=True)\n",
    "\n",
    "# Fix Number of Beterans by mean percentage over mean population applied to total population\n",
    "df['Number of Veterans'].fillna((df['Total Population']*df['Number of Veterans'].mean()/df['Total Population'].mean()).astype(int), inplace=True)\n",
    "\n",
    "# Fix Foreign Born by mean percentage over mean population applied to total population\n",
    "df['Foreign-born'].fillna((df['Total Population']*df['Foreign-born'].mean()/df['Total Population'].mean()).astype(int), inplace=True)\n",
    "\n",
    "# Fix Male Population by mean percentage over mean population applied to total population\n",
    "df['Male Population'].fillna((df['Total Population']*df['Male Population'].mean()/df['Total Population'].mean()).astype(int), inplace=True)\n",
    "\n",
    "# Fix Female Population as total population minus estimated Male Population\n",
    "df['Female Population'].fillna((df['Total Population']-df['Male Population']).astype(int), inplace=True)\n",
    "\n",
    "# Create CityCode unique identifier\n",
    "df['city_code']= df[['City', 'State Code']].agg('_'.join, axis=1)\n",
    "\n",
    "# Create df with compatible column names\n",
    "df_city = df.rename(columns = {'Median Age': 'median_age', 'Male Population': 'male_population','Female Population': 'female_population'\n",
    "                              ,'Total Population': 'total_population','Number of Veterans': 'number_of_veterans','Foreign-born': 'foreign_born',\n",
    "                               'Average Household Size': 'average_household_size','State Code': 'state_code'}, inplace = False)\n",
    "\n",
    "# Create df for population based on df_city columns\n",
    "df_population = pd.DataFrame()\n",
    "df_population['city_code']=df_city['city_code']\n",
    "df_population['race']=df_city['Race']\n",
    "df_population['count']=df_city['Count']\n",
    "df_population=df_population.drop_duplicates()\n",
    "\n",
    "# Write to S3 Parquet file\n",
    "population_table = spark.createDataFrame(df_population)\n",
    "population_table.write.mode('overwrite').parquet(output_data+\"population/population.parquet\")\n",
    "\n",
    "# Remove not needed columns and duplicates\n",
    "\n",
    "df_city.drop('Race',  axis='columns', inplace=True)\n",
    "df_city.drop('Count',  axis='columns', inplace=True)\n",
    "df_city=df_city.drop_duplicates()\n",
    "\n",
    "# Write to S3 Parquet file\n",
    "cities_table = spark.createDataFrame(df_city)\n",
    "cities_table.write.mode('overwrite').parquet(output_data+\"cities/cities.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Data Wrangling AIRPORTS file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "df_air = pd.read_csv('airport-codes_csv.csv')\n",
    "\n",
    "\n",
    "# Split coordinates\n",
    "new_coordinates = df_air[\"coordinates\"].str.split(\",\", n = 1, expand = True)\n",
    "\n",
    "# Create 2 dimension coordinates\n",
    "df_air[\"latitude\"]= new_coordinates[1].astype(float).round(2)\n",
    "df_air[\"longitude\"]= new_coordinates[0].astype(float).round(2)\n",
    "\n",
    "# Adapt format to temperatures file\n",
    "df_air['latitude'] = [str(x)+'N' if x > 0 else str(x*-1)+'S' for x in df_air['latitude']]\n",
    "df_air['longitude'] = [str(x)+'N' if x > 0 else str(x*-1)+'W' for x in df_air['longitude']]\n",
    "\n",
    "# Adapt format to elevation_ft\n",
    "df_air['elevation_ft'] = [str(x) for x in df_air['elevation_ft']]\n",
    "\n",
    "# Delete original coordinates\n",
    "del df_air['coordinates']\n",
    "\n",
    "# Pending development\n",
    "df_air.head()\n",
    "\n",
    "# Only US Airports\n",
    "\n",
    "df_air=df_air[df_air['iso_country']=='US']\n",
    "\n",
    "# Review Not Nulls: ident should be populated to iata_code and local_code\n",
    "#df_air[df_air.notnull().any(axis=1)]\n",
    "df_air['gps_code'] = df_air['ident']\n",
    "df_air['iata_code'] = df_air['ident']\n",
    "df_air['local_code'] = df_air['ident']\n",
    "\n",
    "# Clean 4 digit iata_code (remove leading K)\n",
    "df_air[\"iata_code\"]= [str(x)[-3:] for x in df_air['iata_code']]\n",
    "\n",
    "# Set default elevation to 0 for NaN\n",
    "df_air['elevation_ft'].fillna('0.0', inplace=True)\n",
    "\n",
    "# Set default municipalty as name for NaN\n",
    "df_air['municipality'].fillna(\"(\"+str(df_air['name'])+\")\", inplace=True)\n",
    "\n",
    "# Define State Code from iso_region\n",
    "df_air[\"state_code\"]= [str(x)[-2:] for x in df_air['iso_region']]\n",
    "\n",
    "# Define city Code\n",
    "df_air[\"city_code\"]= df_air[['municipality', 'state_code']].agg('_'.join, axis=1)\n",
    "\n",
    "# Remove unused columns and duplicates\n",
    "df_air.drop('continent',  axis='columns', inplace=True)\n",
    "df_air.drop_duplicates()\n",
    "\n",
    "# Write to S3 Parquet file\n",
    "airports_table = spark.createDataFrame(df_air)\n",
    "airports_table.write.mode('overwrite').parquet(output_data+\"airports/airports.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Data Wrangling IMMIGRATION file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load SAS_DATA from RAW parquet file\n",
    "df2=spark.read.parquet(\"sas_data\")\n",
    "\n",
    "# Cast cicid to Integer\n",
    "df2 = df2.withColumn(\"cicid\",df2[\"cicid\"].cast(IntegerType()))\n",
    "\n",
    "# Cast i94yr (Year) to Integer\n",
    "df2 = df2.withColumn(\"i94yr\",df2[\"i94yr\"].cast(IntegerType()))\n",
    "\n",
    "# Cast i94mon (Month) to Integer\n",
    "df2 = df2.withColumn(\"i94mon\",df2[\"i94mon\"].cast(IntegerType()))\n",
    "\n",
    "# Cast i94cit (Country Code SAS) to Integer\n",
    "df2 = df2.withColumn(\"i94cit\",df2[\"i94cit\"].cast(IntegerType()))\n",
    "\n",
    "# Retrive i94cit description from SAS Dictionary (Country Code SAS)\n",
    "def getSASAny(dic, key, default=None):\n",
    "    if key in dic:\n",
    "        return dic[key]\n",
    "    return default\n",
    "\n",
    "get_i94cit = udf(lambda x: getSASAny(i94cntyl,x))\n",
    "df2 = df2.withColumn(\"descI94cit\",get_i94cit(df2.i94cit))\n",
    "\n",
    "# Cast i94res (Country Code SAS) to Integer\n",
    "df2 = df2.withColumn(\"i94res\",df2[\"i94res\"].cast(IntegerType()))\n",
    "\n",
    "# Retrive i94res description from SAS Dictionary (Country Code SAS)\n",
    "def getSASAny(dic, key, default=None):\n",
    "    if key in dic:\n",
    "        return dic[key]\n",
    "    return default\n",
    "\n",
    "get_i94res = udf(lambda x: getSASAny(i94cntyl,x))\n",
    "df2 = df2.withColumn(\"descI94res\",get_i94cit(df2.i94res))\n",
    "\n",
    "# Lookup Dictionary from SAS for i94port value and add to dataframe\n",
    "def getSASAny(dic, key, default=None):\n",
    "    if key in dic:\n",
    "        return dic[key]\n",
    "    return default\n",
    "\n",
    "get_i94port = udf(lambda x: getSASAny(i94prtl,x))\n",
    "df2 = df2.withColumn(\"newI94port\",get_i94port(df2.i94port))\n",
    "\n",
    "# Lookup Dictionary from SAS for i94mode value and add to dataframe (default 9 = 'Not reported')\n",
    "def getSASAny(dic, key, default=None):\n",
    "    if key in dic:\n",
    "        return dic[key]\n",
    "    return default\n",
    "get_i94mode = udf(lambda x: getSASAny(i94model,x,'Not reported'))\n",
    "df2 = df2.withColumn(\"descrI94mode\",get_i94mode(df2.i94mode))\n",
    "\n",
    "# Lookup Dictionary from SAS for i94addr value and add to dataframe (default 99= 'All Other Codes')\n",
    "def getSASAny(dic, key, default=None):\n",
    "    if key in dic:\n",
    "        return dic[key]\n",
    "    return default\n",
    "get_i94addr = udf(lambda x: getSASAny(i94addrl,x,'All Other Codes'))\n",
    "\n",
    "df2 = df2.withColumn(\"descrI94addr\",get_i94addr(df2.i94addr))\n",
    "\n",
    "#Update i94addr to 99 code for non-matching items with SAS Dictionary\n",
    "df2=df2.withColumn('i94addr',sf.when(df2.descrI94addr == 'All Other Codes',99).otherwise(df2.i94addr))\n",
    "\n",
    "# Cast arrdate (SAS Date Format) to Date String\n",
    "def sas_dtnum_to_date (days):\n",
    "    t0 = datetime.date(year = 1960, month = 1, day = 1)\n",
    "    return (str(t0 + datetime.timedelta(days)))\n",
    "sas_date = udf(lambda x: sas_dtnum_to_date(x))\n",
    "df2 = df2.withColumn(\"stdArrdate\",sas_date(df2.arrdate))\n",
    "\n",
    "# Cast depdate (SAS Date Format) to Date String\n",
    "def sas_dtnum_to_date2 (days):\n",
    "    t0 = datetime.date(year = 1960, month = 1, day = 1)\n",
    "    if days==None:\n",
    "        return None\n",
    "    else:\n",
    "        return (str(t0 + datetime.timedelta(days)))\n",
    "sas_date2 = udf(lambda x: sas_dtnum_to_date2(x))\n",
    "df2 = df2.withColumn(\"stdDepdate\",sas_date2(df2.depdate))\n",
    "\n",
    "# Store in parquet File Partitioned by StateCode, Year, Month\n",
    "df2.write.mode('append').partitionBy(\"i94addr\",\"i94yr\", \"i94mon\").parquet(output_data+\"immigrations/immigrations.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Generate Auxiliar Time Table with Dates from Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Time Table Aux for Data Analisys (change to final names)\n",
    "df_date_arr=spark.createDataFrame(df2.select('stdArrdate').distinct().collect())\n",
    "df_date_dep=spark.createDataFrame(df2.select('stdDepdate').distinct().collect())\n",
    "\n",
    "# Join both dates into a single column\n",
    "df_dates=df_date_dep.union(df_date_arr)\n",
    "\n",
    "# Create dummy date record to ensure parquet file exists\n",
    "initial_date = [(\"1960-01-01\",\"01\",\"01\",\"01\",\"1960\",\"01\")]\n",
    "date_cols = [\"dateEvent\",\"day\",\"week\",\"month\",\"year\",\"weekday\"]\n",
    "dateevents_table = spark.createDataFrame(data=initial_date, schema = date_cols)\n",
    "dateevents_table.write.mode('append').parquet(output_data+\"dateevents/dateevents.parquet\")\n",
    "\n",
    "# Load existing Dates from Parquet\n",
    "dateevents_table=spark.read.parquet(output_data+\"dateevents/dateevents.parquet\")\n",
    "\n",
    "# Select only DateEvent Column from dataframe\n",
    "dateevents_table=spark.createDataFrame(dateevents_table.select('dateEvent').distinct().collect())\n",
    "\n",
    "# Integrate existing dates and new dates into a single column\n",
    "dateevents_table=dateevents_table.union(df_dates)\n",
    "\n",
    "# Consolidate dates with distinct clause\n",
    "dateevents_table=spark.createDataFrame(dateevents_table.distinct().collect())\n",
    "\n",
    "# Calculate date derivated fields into datafram\n",
    "dateevents_table = dateevents_table.select(dateevents_table.dateEvent,dayofmonth('dateEvent').alias('day'),\n",
    "                                           weekofyear('dateEvent').alias('week'),month('dateEvent').alias('month'),\n",
    "                                                 year('dateEvent').alias('year'),dayofweek('dateEvent').alias('weekday'))\n",
    "\n",
    "#overwrite results in parquet\n",
    "dateevents_table.write.mode('overwrite').parquet(output_data+\"dateevents/dateevents.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Data Quality (x2) for Temperatures reported from cities not properly coded in Cities Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Check integrity of Temperatures vs Cities\n",
    "\n",
    "# Load existing Temperatures Records from Parquet\n",
    "temperatures_check_table=spark.read.parquet(output_data+\"temperatures/temperatures.parquet\")\n",
    "temperatures_check_table.createGlobalTempView(\"temperature_check\")\n",
    "\n",
    "# Load existing Cities Records into a tempView\n",
    "cities_check_table=spark.read.parquet(output_data+\"cities/cities.parquet\")\n",
    "cities_check_table.createGlobalTempView(\"cities_check\")\n",
    "\n",
    "# Return States without temperatures reported\n",
    "dataQuality1=spark.sql(\"SELECT distinct (state_code) FROM global_temp.cities_check where \\\n",
    "                        state_code not in (SELECT distinct (stateCode) FROM global_temp.temperature_check)\").toPandas()\n",
    "\n",
    "# Return Temperatures reported from cities not in cities tables into a Pandas Dataframe\n",
    "dataQuality2=spark.sql(\"SELECT distinct (cityCode) FROM global_temp.temperature_check where \\\n",
    "                       cityCode not in (SELECT distinct (city_code) FROM global_temp.cities_check)\").toPandas()\n",
    "\n",
    "spark.catalog.dropGlobalTempView(\"temperature_check\")\n",
    "spark.catalog.dropGlobalTempView(\"cities_check\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
